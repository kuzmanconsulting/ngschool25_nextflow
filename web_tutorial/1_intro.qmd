---
title: "bash and containers"  
---


# Learning objectives

By the end of this workshop you should be able to:

-   see the advantages of Nextflow over bash
-   Write a simple Nextflow **workflow**
-   Have basic understanding of the concepts of **Channels**, **Processes** and **Operators**
-   Run an test nf-core pipeline
-   Know where to go next ðŸ¤·

# The problem

Lets say we want to do the following, as I am sure you have done many times before:

We will start with fastq data and perform trimming, after which we will perform fastqc.

![](images/clipboard-1893698979.png)

## Solution - bash

In bash we would do this:
```{bash}
fastp 
```

But of course, it will fail since we do not have fastp and fastqc installed.

::: callout-tip
Good practice is to use containers which will enable reproducibility. Building dockerfiles and images is sometimes tricky, but also could be defying the purpose of reproducible research. You can find many already available dockers/singularity images at [dockerhub](https://hub.docker.com/r/biocontainers/) and [quay.io](https://quay.io) .
:::

## Use containers!

We will use the docker for fastp and fastqc available on dockerhub.

First; pull the container locally:

```{r}
docker pull biocontainers/fastp:v0.20.1_cv1
```

Next, enter the container image:

```{r}  

docker run -v .:/data -it biocontainers/fastp:v0.20.1_cv1 /bin/bash

# exit the container with exit
```

::: {.callout-note appearance="minimal"}
Explanation

**`docker run`** start a new container from a specified image.

**`-v .:/data`** mounts a volume. Here, the current directory (represented by .) on your host machine is mapped to the /data directory inside the container.

**`-i (interactive) t (tty)`** provides a terminal session inside the container.\
Together, they allow you to run the container in interactive mode, making it easier to execute commands and see outputs.

**`biocontainers/fastp:v0.20.1_cv1`** docker being used. image:tag.

**`/bin/bash`** which command to run within a docker.
:::

Once you entered the container image, the fastp program exists and we all have the same version. Moreover, we all are running it on the same version of the OS. Check:

```{bash, eval=FALSE}
cat /etc/os-release
fastp --version
```

```         
NAME="Ubuntu" VERSION="16.04.6 LTS (Xenial Xerus)"\
ID=ubuntu\
ID_LIKE=debian\
PRETTY_NAME="Ubuntu 16.04.6 LTS"\
VERSION_ID="16.04"\
HOME_URL="http://www.ubuntu.com/"\
SUPPORT_URL="http://help.ubuntu.com/"\
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"\
VERSION_CODENAME=xenial\
UBUNTU_CODENAME=xenial

fastp 0.20.1
```

## bash command with containers

So, lets combine bash command for fastp and container:

```{bash, eval=FALSE}
docker run -v .:/data -it biocontainers/fastp:v0.20.1_cv1 fastp \
      --in1 data/liver_1.fq \
      --in2 data/liver_2.fq \
      --out1 data/liver_1.trimmed.fq \
      --out2 data/liver_2.trimmed.fq \
      -h liver.html 

```

::: callout-exercise
## ðŸ’¡ Exercise: task1

Try it yourself! 1. Pull the container biocontainers/fastqc:v0.11.9_cv8 2. Use the fastqc on data/liver_1.fq
:::

```{bash, eval=FALSE, task1}

#pull the container  
...

# run the command:  
docker run -v .:/data -it ... ...

```

Solution:

```{bash, eval=FALSE, solution-task1}
#| code-fold: true

docker pull biocontainers/fastqc:v0.11.9_cv8
docker run -v .:/data -it biocontainers/fastqc:v0.11.9_cv8 fastqc data/liver_1.fq 
```

# bash scripts as a pipeline

In theory, we can create a bash 'pipeline' - script to process each sample with both steps.  

```{bash, eval=FALSE}
#| code-fold: true

# we are in the ngschool25_nextflow/web_tutorial directory

cd .. #to ngschool25_nextflow
docker run -v .:/data -it biocontainers/fastp:v0.20.1_cv1 fastp \
      --in1 data/liver_1.fq \
      --in2 data/liver_2.fq \
      --out1 data/liver_1.trimmed.fq \
      --out2 data/liver_2.trimmed.fq \
      -h liver.html 

docker run -v .:/data -it biocontainers/fastqc:v0.11.9_cv8 fastqc data/liver_1.fq 
```

So the script would look something like this:  

[full_code.sh](full_code.sh)

Run it on liver samples:  

``` {bash}
# move to the ngschool25_nextflow directory
# if cou run from the web_tutorial directory, the container will mount the wrong folder to the volume...  

# if needed change permission to run:
chmod +x code/full_code.sh
code/full_code.sh ../data/liver_1.fq ../data/liver_1.fq 
```

## output locations 

We need to take care of where the outputs will be located.  

## logging  

We need to manually create logs if we want to know what happened. 

## memory, threads

 If we want to allocate different amount of memory and CPUs to each of the processes we make, we need to create separate bash scripts for each process, and define the threads and memory separately.

## process dependencies  

Next, we need to define dependencies - we will have to wait for one script to finish and then the other one can start running. This will depend on where we run the scripts - if we run it locally in bash, we can use wait; if we run it in slurm/pbs/torque, we need to take care of dependencies ourselves.

## parallelism  

We need to take care of writing the bash "pipeline" to enable paralel execution. Also we need to take care of running the scripts in parallel on multiple samples. 

## resume  

We need to take care of status of each of the runs. If some sample fails in one step, we need to find it manually to understand what and where failed. 

We need to resume the execution of the pipeline manually when some step fails in some sample.  

## upscaling to production level

The additional problems arise when you want to go to production-level, and run this pipeline on thousands of samples. You will want to use the cloud for this, and will need to implement the pipeline to work on the cloud.

## documentation  

We need to take care of the documentation. What samples were ran? How do we know if they all ran as expected?  With which version of the code were they run? We need to keep track of that manually.  