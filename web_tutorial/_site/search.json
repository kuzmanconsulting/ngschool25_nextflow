[
  {
    "objectID": "2_channels.html",
    "href": "2_channels.html",
    "title": "Nextflow intro",
    "section": "",
    "text": "Nextflow takes care of all of those problems for us. It is both an workflow orchestrator and a coding language.\n\n\n\n\n\n\nkey terms\n\n\n\nworkflow: pipeline in nextflow is called a workflow\nchannel: structure which transfers data between steps in the pipeline\nprocess: task that will happen on data\n\n\n\n\nNextflow workflow is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).\n\nProcesses are executed independently and are isolated from each other. The only way they can communicate is via ‚Äúchannels‚Äù, asynchronous first-in, first-out (FIFO) queues. In other words, every input and output of a process is represented as a channel. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these input and output declarations.\n\n\n\nFirst nextflow script\n\nmkdir tests\ncd tests\n\nnextflow run ../code/my_first_script.nf \\\n   -c ../code/config.nf \n\n\n\n\n\n\n\nExplanation:\n\n\n\nnextflow run code/my_first_script.nf runs a nextflow script code/my_first_script.nf\n-c config.nf uses configuration file for nextflow run. Parameters which are nextflow-related are handed with a single dash.\n\n\n\n\n\n\nnextflow.enable.dsl=2 # (Domain-Specific Language version 2)\n\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n\n  input:\n  path infilename\n\n  output:\n  path '*' \n      \n  script:\n  \"\"\"\n  fastqc ${infilename}  \n  \"\"\"\n}\n\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromPath( \"../../data/liver_1.fq\" )\n\n  // run FASTQC:\n  FASTQC(infile_channel)\n\n}\n\n\nOf course, there is no need to hard code inputs to a channel. Some usual options for defining inputs can be:\nTo view a content of a channel, you can use method view() For example, we created code/script1.nf where we only created a channel from a file. To view it, just add .view() after a channel.\n\n# code/script1.nf\nworkflow {\n\n  // parse input:\n  infile_channel_1 = \n    Channel.fromPath( \"../../data/liver_*.fq\" )\n    .view()\n\n}\n\n\nWe did this in script1 . So lets run it and see what we get:\n\nnextflow run ../code/script1.nf    -c ../code/config.nf \n\n\n\n\nWhat will happen if you try to run the previous script with ‚Äú../../data/liver_*.fq‚Äù instead of ‚Äú../../data/liver_1.fq‚Äù also add infile_channel.view() . Try to run it yourself!\n\n\ncp ../code/my_first_script.nf ../code/script2.nf\n# change ../code/script2.nf and run it.\n# 1. change \n# add .view() to view output of the channel!\n\nnextflow run ../code/script2.nf  -c ../code/config.nf \n\n\n\n\n\n\n\nüí° Solution:\n\n\n\n\n\nCode\n\ncp ../code/my_first_script.nf ../code/script2.nf\n# change ../code/script2.nf and run it.\nsed  -i 's|../../data/liver_1.fq\" )|../../data/liver_*.fq\" ).view()|g' ../code/script2.nf\n\nnextflow run ../code/script2.nf  -c ../code/config.nf \n\n\n\n\nNow fastqc is ran 2 times!\n\n\n\n\n\n\nTip\n\n\n\nThere are many ways to create a channel in nextflow, I advise you to look at the official documentation on channel factories for more information.\n\n\n\n\nNow we created multiple files as results, but they are all somewhere around in the work directory. You can define publishDir to have the resuls all in one place.\nAdd the following line to the process, or config:\n\n  publishDir \"results\"\n\nIf you want to collect only html files, you can specify this in the output by :\n\n  output \"*html\"\n\n\n\n\nWe have now repeated the fastqc computation a lot of times already. There is a neat option in nextflow which allowes you to save computation time and repeat only the steps which have changed. you can activate it by running nextflow command with -resume tag.\n\n\nnextflow run ../code/script2.nf  -c ../code/config.nf -resume\n\nThis way only the steps we change will be ran again, the rest will be found in cache.\n\n\n\nOf course, you don‚Äôt need to hard code the path, you can use variable that will store a path of the file/files you want to input.\nin the workflow, you can simply replace the actual file path with a parameter defined in params.nameoftheparameter, where nameoftheparameter can be any variable name you choose.\n\n# in the workflow:  \n#  // parse input:\n  infile_channel = Channel.fromPath( params.infile )\n                    .view()\n\nNeat thing about this is that you can define the parameter directly when you call the nextflow run:\n\n\n\n\n\n\nTip\n\n\n\nto define a parameter while callling a nextflow run, simply use double dash: ‚Äò--‚Äô followed by the name of the parameter.\nfor example, to specify a param.infile in workflow, we would need to call\nnextflow run --infile \"path/to/file\" .\nAlteernatively, you can create a config file and store your parameters there! - neat for keeping track of runs!\n\n\n\nTry it out!\n\n\nnextflow run ../code/script2.nf  -c ../code/config.nf -resume --infile \"../../data/*fq\"\n\n\n\n\nThere are many ways to input a file to channel. In bioinformatics, there is often a need to input pairs of files. So, in nextflow there is a special operator for this fromFilePairs .\nLets try this out in script 3:\n\ncp ../code/script2.nf ../code/script3.nf\n\nOpen and change script3:\n\n\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromFilePairs( params.infile )\n                    .view()\n \n}\n\nand call it with:\n\n\nnextflow run ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\"\n\nThis creates a channel which is a tuple of two elements. First one is the sample ID, and second one is another tuple of two elements, which are paths to file_1.fq and file_2.fq.\nIt is easy to access the individual elements, you can do it in the following way:\nIn the process, it is easy to parse this, just instead of path infilename use:\n\n\ntuple val(sampleid),path(infiles)\n\nNow we can access individual file from infiles with ${infiles[0]} ${infiles[1]}, and we know the sampleid, it is saved as variable ${sampleid}.\nAnother neat thing you can do is to tag the process execution by some name, lets see this by using ‚Äútag‚Äù:\n\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n**  tag \"running on $sampleid\" **\n\n  input:\n  tuple val(sampleid),path(infiles)\n  \n  output:\n  path '*html' \n  publishDir \"results\"\n\n  \n  script:\n  \"\"\"\n  fastqc ${infiles[0]} ${infiles[1]}\n  \"\"\"\n}\n\n\nOK, cool! We are ready for some exercise!!\n\n\n\nAdd a parameter ‚Äúoutdir‚Äù to specify where the output location is for the final nextflow results.\n\n\n\n\n\n\n\nüí° Solution: Add a parameter !\n\n\n\nAdding the parameter to script:\n\n\nCode\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n  tag \"$sampleid\"\n\n  input:\n  tuple val(sampleid),path(infiles)\n  \n  output:\n  path '*html' \n  publishDir params.outdir\n\n  \n  script:\n  \"\"\"\n  fastqc ${infiles[0]} ${infiles[1]} \n  \"\"\"\n}\n\n\nRunning it with outdir:\n\n\nCode\nnextflow run ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme \n\n\n\n\n\n\nOK, we are ready for a bigger bite!\nCreate a new process, TRIM that will trim the fastq files as we did before: Use appropriate variables as needed!\nRemember, we used the container: 'biocontainers/fastp:v0.20.1_cv1'.\nThis is what we have from before:\n\n\nfastp\\\n  --in1 data/liver_1.fq\\\n  --in2 data/liver_2.fq\\\n  --out1 data/liver_1.trimmed.fq\\\n  --out2 data/liver_2.trimmed.fq\\\n  -h liver.html \n\n\n\n\n\n\n\n\nüí° Solution: Add a trimming step!\n\n\n\nAdding the TRIM process to script:\n\n\nCode\nprocess TRIM {\n    \n  container 'biocontainers/fastp:v0.20.1_cv1'\n  tag \"$sampleid\"\n\n  input:\n  tuple val(sampleid),path(infiles)\n  \n  output:\n  path '*' \n  publishDir params.outdir\n\n  \n  script:\n  \"\"\"\n  fastp\\\n  --in1 ${infiles[0]}\\\n  --in2 ${infiles[1]}\\\n  --out1 ${sampleid}_1.trimmed.fq\\\n  --out2 ${sampleid}_2.trimmed.fq\\\n  -h ${sampleid}.html \n\n  \"\"\"\n}\n\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromFilePairs( params.infile )\n                    .view()\n  // run FASTQC:\n  FASTQC(infile_channel) \n  trimmed_channel = TRIM(infile_channel).out\n  FASTQC(trimmed_channel) \n\n}\n\n\nRunning it :\n\n\nCode\nnextflow run ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme \n\n\n\n\n\n\nIf you want extra work, create additional process, FASTQC_SINGLE. Change pthe parameters of FASTQC to be able to input and process only a single file. Run that process on the output of TRIM step.\n\n\n\n\n\n\n\nüí° Solution: Add a new FASTQC_SINGLE step!\n\n\n\n\n\nCode\nprocess FASTQC_SINGLE {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n  tag \"$sampleid\"\n\n  input:\n  path infile\n  \n  output:\n  path '*html' \n  publishDir params.outdir\n\n  \n  script:\n  \"\"\"\n  fastqc ${infile}\n  \"\"\"\n}\n\n\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromFilePairs( params.infile )\n                    .view()\n  // run FASTQC:\n  FASTQC(infile_channel) \n  trimmed_channel = TRIM(infile_channel)\n  FASTQC_SINGLE(trimmed_channel.trimmed) \n\n}\n\n\nRunning it :\n\n\nCode\nnextflow run ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme \n\n\n\n\n\n\nWant to see something quite cool?\nEvery nextflow run automatically loggs all of its progress. The main log from, the last run is always saved as .nextflow.log. Check it out!\n\ncat .nextflow.log  \n\nThere are many information, most useful might be seeing per process status:\n\ncat .nextflow.log | grep COMPLETED  \n\nWhatever is completed successfully has the exit status 0. So if you grep everything which does not have exit status 0 you will find all failed processes:\n\n cat .nextflow.log | grep COMPLETED | grep -v \"exit: 0\"\n\nJust adding flags -with-report -with-dag -with-timeline -with-trace enable reporrts on the pipeline generated automatically.\n\nrun ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme -with-report -with-dag -with-timeline -with-trace\n\n\n\n\n\n\n\n\nNextflow automatically draws a graph of the workflow!\n\n\n\nNextflow tracks all progress for each run! You can easily get information on how much CPUs or memory was actually used in a process, and get a lot of additional details on where the processes were run.\n\n\nEvery process working directory is available. Check one in the report and enter it. In there, you have :\n\n\n\n\n\n\nImportant files\n\n\n\n.command.run: File that sets the stage. It configures the containers and prepares the environment to run the process.\n.command.sh: The actual script being run. If you need to change something, best way is to change the .sh and run with .command.run\n.command.err: Errors outputted by the process.\n.command.log: Log outputted by the process.\n.command.out: Any standard output outputted by the process.\n.exitcode: If 0, everything is ok.\n\n\n\n\n\n\n\n( This might be the most useful exercise of all.. )\n\n\nRun the following code and fix it .\n\n\nnextflow run ../code/script4.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme -with-report -with-dag -with-timeline -with-trace\n\n\n\n\n\n\n\n\nüí° Solution: Fix the pipeline!\n\n\n\n\n\nCode\n#If you run \n\ncat .nextflow.log | grep COMPLETED | grep -v \"exit: 0\"\n\n#Or check the execution report, you will find which working directory was problematic.  \n#In there, you can check out .command.err and see that fastp command wasnt found. \n# We were supposed to do fastqc in this step, so no wonder. \n# change the fastp to fastqc in the .command.sh, and run the .command.run to verify the problem is solved.\n\nsed -i \"\" \"s|fastp|fastqc|g\" .command.sh\n# change permission to be able to run .command.run\n\nchmod +x .command.run\n./.command.run\n\n#Works! \n\n#now change in the original pipeline and run with -resume!",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#processes-and-channels",
    "href": "2_channels.html#processes-and-channels",
    "title": "Nextflow intro",
    "section": "",
    "text": "Nextflow workflow is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).\n\nProcesses are executed independently and are isolated from each other. The only way they can communicate is via ‚Äúchannels‚Äù, asynchronous first-in, first-out (FIFO) queues. In other words, every input and output of a process is represented as a channel. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these input and output declarations.",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#my-first-nextflow-script",
    "href": "2_channels.html#my-first-nextflow-script",
    "title": "Nextflow intro",
    "section": "",
    "text": "First nextflow script\n\nmkdir tests\ncd tests\n\nnextflow run ../code/my_first_script.nf \\\n   -c ../code/config.nf \n\n\n\n\n\n\n\nExplanation:\n\n\n\nnextflow run code/my_first_script.nf runs a nextflow script code/my_first_script.nf\n-c config.nf uses configuration file for nextflow run. Parameters which are nextflow-related are handed with a single dash.",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#script-walkthrough",
    "href": "2_channels.html#script-walkthrough",
    "title": "Nextflow intro",
    "section": "",
    "text": "nextflow.enable.dsl=2 # (Domain-Specific Language version 2)\n\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n\n  input:\n  path infilename\n\n  output:\n  path '*' \n      \n  script:\n  \"\"\"\n  fastqc ${infilename}  \n  \"\"\"\n}\n\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromPath( \"../../data/liver_1.fq\" )\n\n  // run FASTQC:\n  FASTQC(infile_channel)\n\n}\n\n\nOf course, there is no need to hard code inputs to a channel. Some usual options for defining inputs can be:\nTo view a content of a channel, you can use method view() For example, we created code/script1.nf where we only created a channel from a file. To view it, just add .view() after a channel.\n\n# code/script1.nf\nworkflow {\n\n  // parse input:\n  infile_channel_1 = \n    Channel.fromPath( \"../../data/liver_*.fq\" )\n    .view()\n\n}\n\n\nWe did this in script1 . So lets run it and see what we get:\n\nnextflow run ../code/script1.nf    -c ../code/config.nf",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-task2",
    "href": "2_channels.html#exercise-task2",
    "title": "Nextflow intro",
    "section": "",
    "text": "What will happen if you try to run the previous script with ‚Äú../../data/liver_*.fq‚Äù instead of ‚Äú../../data/liver_1.fq‚Äù also add infile_channel.view() . Try to run it yourself!",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-add-a-trimming-step",
    "href": "2_channels.html#exercise-add-a-trimming-step",
    "title": "Nextflow intro",
    "section": "",
    "text": "OK, we are ready for a bigger bite!\nCreate a new process, TRIM that will trim the fastq files as we did before: Use appropriate variables as needed!\nRemember, we used the container: 'biocontainers/fastp:v0.20.1_cv1'.\nThis is what we have from before:\n\n\nfastp\\\n  --in1 data/liver_1.fq\\\n  --in2 data/liver_2.fq\\\n  --out1 data/liver_1.trimmed.fq\\\n  --out2 data/liver_2.trimmed.fq\\\n  -h liver.html",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "bash and containers",
    "section": "",
    "text": "By the end of this workshop you should be able to:\n\nsee the advantages of Nextflow over bash\nWrite a simple Nextflow workflow\nHave basic understanding of the concepts of Channels, Processes and Operators\nRun an test nf-core pipeline\nKnow where to go next ü§∑",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#solution---bash",
    "href": "1_intro.html#solution---bash",
    "title": "bash and containers",
    "section": "Solution - bash",
    "text": "Solution - bash\nIn bash we would do this:\n\nfastp \n\nBut of course, it will fail since we do not have fastp and fastqc installed.\n\n\n\n\n\n\nTip\n\n\n\nGood practice is to use containers which will enable reproducibility. Building dockerfiles and images is sometimes tricky, but also could be defying the purpose of reproducible research. You can find many already available dockers/singularity images at dockerhub and quay.io .",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#use-containers",
    "href": "1_intro.html#use-containers",
    "title": "bash and containers",
    "section": "Use containers!",
    "text": "Use containers!\nWe will use the docker for fastp and fastqc available on dockerhub.\nFirst; pull the container locally:\n\ndocker pull biocontainers/fastp:v0.20.1_cv1\n\nNext, enter the container image:\n\ndocker run -v .:/data -it biocontainers/fastp:v0.20.1_cv1 /bin/bash\n\n# exit the container with exit\n\n\n\n\n\n\n\nExplanation\ndocker run start a new container from a specified image.\n-v .:/data mounts a volume. Here, the current directory (represented by .) on your host machine is mapped to the /data directory inside the container.\n-i (interactive) t (tty) provides a terminal session inside the container.\nTogether, they allow you to run the container in interactive mode, making it easier to execute commands and see outputs.\nbiocontainers/fastp:v0.20.1_cv1 docker being used. image:tag.\n/bin/bash which command to run within a docker.\n\n\n\nOnce you entered the container image, the fastp program exists and we all have the same version. Moreover, we all are running it on the same version of the OS. Check:\n\ncat /etc/os-release\nfastp --version\n\nNAME=\"Ubuntu\" VERSION=\"16.04.6 LTS (Xenial Xerus)\"\\\nID=ubuntu\\\nID_LIKE=debian\\\nPRETTY_NAME=\"Ubuntu 16.04.6 LTS\"\\\nVERSION_ID=\"16.04\"\\\nHOME_URL=\"http://www.ubuntu.com/\"\\\nSUPPORT_URL=\"http://help.ubuntu.com/\"\\\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\\\nVERSION_CODENAME=xenial\\\nUBUNTU_CODENAME=xenial\n\nfastp 0.20.1",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#bash-command-with-containers",
    "href": "1_intro.html#bash-command-with-containers",
    "title": "bash and containers",
    "section": "bash command with containers",
    "text": "bash command with containers\nSo, lets combine bash command for fastp and container:\n\ndocker run -v .:/data -it biocontainers/fastp:v0.20.1_cv1 fastp \\\n      --in1 data/liver_1.fq \\\n      --in2 data/liver_2.fq \\\n      --out1 data/liver_1.trimmed.fq \\\n      --out2 data/liver_2.trimmed.fq \\\n      -h liver.html",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#exercise-task1",
    "href": "1_intro.html#exercise-task1",
    "title": "bash and containers",
    "section": "üí° Exercise: task1",
    "text": "üí° Exercise: task1\nTry it yourself! 1. Pull the container biocontainers/fastqc:v0.11.9_cv8 2. Use the fastqc on data/liver_1.fq",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#output-locations",
    "href": "1_intro.html#output-locations",
    "title": "bash and containers",
    "section": "output locations",
    "text": "output locations\nWe need to take care of where the outputs will be located.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#logging",
    "href": "1_intro.html#logging",
    "title": "bash and containers",
    "section": "logging",
    "text": "logging\nWe need to manually create logs if we want to know what happened.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#memory-threads",
    "href": "1_intro.html#memory-threads",
    "title": "bash and containers",
    "section": "memory, threads",
    "text": "memory, threads\nIf we want to allocate different amount of memory and CPUs to each of the processes we make, we need to create separate bash scripts for each process, and define the threads and memory separately.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#process-dependencies",
    "href": "1_intro.html#process-dependencies",
    "title": "bash and containers",
    "section": "process dependencies",
    "text": "process dependencies\nNext, we need to define dependencies - we will have to wait for one script to finish and then the other one can start running. This will depend on where we run the scripts - if we run it locally in bash, we can use wait; if we run it in slurm/pbs/torque, we need to take care of dependencies ourselves.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#parallelism",
    "href": "1_intro.html#parallelism",
    "title": "bash and containers",
    "section": "parallelism",
    "text": "parallelism\nWe need to take care of writing the bash ‚Äúpipeline‚Äù to enable paralel execution. Also we need to take care of running the scripts in parallel on multiple samples.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#resume",
    "href": "1_intro.html#resume",
    "title": "bash and containers",
    "section": "resume",
    "text": "resume\nWe need to take care of status of each of the runs. If some sample fails in one step, we need to find it manually to understand what and where failed.\nWe need to resume the execution of the pipeline manually when some step fails in some sample.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#upscaling-to-production-level",
    "href": "1_intro.html#upscaling-to-production-level",
    "title": "bash and containers",
    "section": "upscaling to production level",
    "text": "upscaling to production level\nThe additional problems arise when you want to go to production-level, and run this pipeline on thousands of samples. You will want to use the cloud for this, and will need to implement the pipeline to work on the cloud.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#documentation",
    "href": "1_intro.html#documentation",
    "title": "bash and containers",
    "section": "documentation",
    "text": "documentation\nWe need to take care of the documentation. What samples were ran? How do we know if they all ran as expected? With which version of the code were they run? We need to keep track of that manually.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-add-outdir-parameter",
    "href": "2_channels.html#exercise-add-outdir-parameter",
    "title": "Nextflow intro",
    "section": "",
    "text": "Add a parameter ‚Äúoutdir‚Äù to specify where the output location is for the final nextflow results.",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-add-a-new-fastqc_single-step",
    "href": "2_channels.html#exercise-add-a-new-fastqc_single-step",
    "title": "Nextflow intro",
    "section": "",
    "text": "If you want extra work, create additional process, FASTQC_SINGLE. Change pthe parameters of FASTQC to be able to input and process only a single file. Run that process on the output of TRIM step.",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#logging",
    "href": "2_channels.html#logging",
    "title": "Nextflow intro",
    "section": "",
    "text": "Want to see something quite cool?\nEvery nextflow run automatically loggs all of its progress. The main log from, the last run is always saved as .nextflow.log. Check it out!\n\ncat .nextflow.log  \n\nThere are many information, most useful might be seeing per process status:\n\ncat .nextflow.log | grep COMPLETED  \n\nWhatever is completed successfully has the exit status 0. So if you grep everything which does not have exit status 0 you will find all failed processes:\n\n cat .nextflow.log | grep COMPLETED | grep -v \"exit: 0\"\n\nJust adding flags -with-report -with-dag -with-timeline -with-trace enable reporrts on the pipeline generated automatically.\n\nrun ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme -with-report -with-dag -with-timeline -with-trace\n\n\n\n\n\n\n\n\nNextflow automatically draws a graph of the workflow!\n\n\n\nNextflow tracks all progress for each run! You can easily get information on how much CPUs or memory was actually used in a process, and get a lot of additional details on where the processes were run.\n\n\nEvery process working directory is available. Check one in the report and enter it. In there, you have :\n\n\n\n\n\n\nImportant files\n\n\n\n.command.run: File that sets the stage. It configures the containers and prepares the environment to run the process.\n.command.sh: The actual script being run. If you need to change something, best way is to change the .sh and run with .command.run\n.command.err: Errors outputted by the process.\n.command.log: Log outputted by the process.\n.command.out: Any standard output outputted by the process.\n.exitcode: If 0, everything is ok.",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-break-the-pipeline",
    "href": "2_channels.html#exercise-break-the-pipeline",
    "title": "Nextflow intro",
    "section": "",
    "text": "This might be the most useful exercise of all..\nLets rename one file to save it, and create an empty file instead of it. Then try to rerun the pipeline to see how difficult it will be to understand what happened.\n\nmv ../../data/liver_1.fq ../../data/liver_1.fq_cp \ntouch ../../data/liver_1.fq\nrm -rf .*\nnextflow run ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme -with-report -with-dag -with-timeline -with-trace",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-fix-a-broken-pipeline",
    "href": "2_channels.html#exercise-fix-a-broken-pipeline",
    "title": "Nextflow intro",
    "section": "",
    "text": "( This might be the most useful exercise of all.. )",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "4_nf-core.html",
    "href": "4_nf-core.html",
    "title": "nf-core",
    "section": "",
    "text": "Ok, ok‚Ä¶\nI agree it got a bit tricky, yes.\nHowever, since all of this is so great and reproducible, there must be tons of people using it and creating reproducible scripts and best practice pipelines for everyone to run!\nWell!",
    "crumbs": [
      "nf-core"
    ]
  },
  {
    "objectID": "4_nf-core.html#what-is-nf-core",
    "href": "4_nf-core.html#what-is-nf-core",
    "title": "nf-core",
    "section": "What is nf-core",
    "text": "What is nf-core",
    "crumbs": [
      "nf-core"
    ]
  },
  {
    "objectID": "4_nf-core.html#scnanoseq-pipeline",
    "href": "4_nf-core.html#scnanoseq-pipeline",
    "title": "nf-core",
    "section": "scnanoseq pipeline",
    "text": "scnanoseq pipeline\nSince later in the week we have a hackathon, and we plan to give it on single cell transcriptomics data, I wanted to see what was available from ‚Äúbest practice‚Äù workflows to analyse this data. One such pipeline is scnanoseq.\nIt is quite easy to run an nf-core pipeline, because those pipelines are readily available on github, and nextflow works great with github, you can just do this:\nmkdir test_scnanoseq\ncd test_scnanoseq\n#nextflow run nf-core/scnanoseq -profile test,docker --outdir . \nnextflow run nf-core/scnanoseq -profile test,singularity --outdir . \nWhile it works, we can check out what the pipeline does:\n\nAs often the case, this pipeline is designed for a specific use case - nanopore long read single cell experiment analysis. Specifically, it supports barcodes 10x_3v3 and 10x_5v2. One of the goals in the hackathon is to adjust and enable this pipeline to be run on real pacbio derived single cell data that we have, and compare results with the ones provided in the paper. Another goal is to change process with a different one.",
    "crumbs": [
      "nf-core"
    ]
  }
]