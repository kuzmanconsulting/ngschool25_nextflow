[
  {
    "objectID": "2_channels.html",
    "href": "2_channels.html",
    "title": "Nextflow intro",
    "section": "",
    "text": "Nextflow takes care of all of those problems for us. It is both an workflow orchestrator and a coding language.\n\n\n\n\n\n\nkey terms\n\n\n\nworkflow: pipeline in nextflow is called a workflow\nchannel: structure which transfers data between steps in the pipeline\nprocess: task that will happen on data\n\n\n\n\nNextflow workflow is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).\n\nProcesses are executed independently and are isolated from each other. The only way they can communicate is via ‚Äúchannels‚Äù, asynchronous first-in, first-out (FIFO) queues. In other words, every input and output of a process is represented as a channel. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these input and output declarations.\n\n\n\nFirst nextflow script\n\nmkdir tests\ncd tests\n\nnextflow run ../code/my_first_script.nf \n\n\n\n\n\n\n\nExplanation:\n\n\n\nnextflow run code/my_first_script.nf runs a nextflow script code/my_first_script.nf\n-c config.nf uses configuration file for nextflow run. Parameters which are nextflow-related are handed with a single dash.\n\n\n\n\n\n\nnextflow.enable.dsl=2 # (Domain-Specific Language version 2)\n\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n\n  input:\n  path infilename\n\n  output:\n  path '*' \n      \n  script:\n  \"\"\"\n  fastqc ${infilename}  \n  \"\"\"\n}\n\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromPath( \"../../data/liver_1.fq\" )\n\n  // run FASTQC:\n  FASTQC(infile_channel)\n\n}\n\n\nTo view a content of a channel, you can use the operator view() For example, we created code/script1.nf where we only created a channel from a file. To view it, just add .view() after a channel.\n\n# code/script1.nf\nworkflow {\n\n  // parse input:\n  infile_channel_1 = \n    Channel.fromPath( \"../../data/liver_*.fq\" )\n    .view()\n\n}\n\n\nWe did this in script1 . So lets run it and see what we get:\n\nnextflow run ../code/script1.nf \n\n\n\nThere are two types of channels in nextflow:\nqueue channels and value channels. Queue channels are expendable, they can only be used once - like when you pipe data in bash. Once used, elements in this queue cannot be re-used. Most channels in nextflow are queue channels. Value channels are long-lived and can be re-used, but they always only hold a single value. They usually hold constant values (or absolute paths).\nThere are many ways to create a channel in nextflow, I advise you to look at the official documentation on channel factories for more information.\n\nResults are saved in working directory, two layers down. There, .html and .zip files are created.\n\n\n\nchange script2.nf to: write liver_*.fq instead of liver_1.fq and view the infile_channel.\nwhen process is ran, the outputs are also gathered into a channel. In other words, FASTQC(infile_channel) is also a channel. View the elements in it!\n\n\n\n\n# change ../code/script2.nf and run it.\n# 1. change liver_1.fq to *fq \n# add .view() to view output of the input channel!\n\n# run the script:  \nnextflow run ../code/script2.nf  \n\n# view the outfile channel and run the script again\n\n\n\n\n\n\n\nüí° Solution:\n\n\n\n\n\nCode\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromPath( \"../../data/liver_*.fq\" )\n                      .view()\n  // run FASTQC:\n  FASTQC(infile_channel)\n    .view()\n\n}\n\nnextflow run ../code/script2.nf  \n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you add -ansi-log false to nextflow run command, you will see all the processes instead of single process per line!\nnextflow run ../code/script2.nf  -ansi-log false\nNow fastqc is ran 2 times!\n\n\n\n\n\n\n\nNow we created multiple files as results, but they are all somewhere around in the work directory. You can define publishDir to have the resuls all in one place.\nAdd the following line to the process:\n\n#process:\n  publishDir \"results\"\n\n#run in bash: \nnextflow run ../code/script2.nf  \n\nNow the results (liver_1_fastqc.html liver_1_fastqc.zip liver_2_fastqc.html liver_2_fastqc.zip) will be saved in the folder: results/ liver_1_fastqc.html liver_1_fastqc.zip liver_2_fastqc.html liver_2_fastqc.zip\n\n\n\nIf you want to collect only html files, you can specify this in the output by :\n\n#process: \n  output:\n  path \"*html\"\n \n# bash:\n#remove the results folder and run the script again:  \nrm -rf results\nnextflow run ../code/script2.nf  \n\nNow the fastqc process only outputs the html files in the channel, and only they are in the results folder.\nWe have seen already that output is a channel. If we want to direct different outputs into different channels, this can be easily done by specifying where the output will be directed to - using the emit keyword.\n\n#in process:  \n\n  output :\n  path \"*html\"\n  path \"*zip\", emit: zip\n\n\n\n\n\n\n\nWarning\n\n\n\nThis will produce an error:\n\n\n# in workflow: \n  FASTQC(infile_channel).view()\n\n# in bash:\n# remove the results folder and run again:\nrm -rf results\nnextflow run ../code/script2.nf  \n\nOutput produces 2 channels, and nextflow doesnt know which one we want. You should specify which one you are interestred in by choosing .html or .zip. So, do this:\n\n\n# in workflow: \n  FASTQC(infile_channel).zip.view()\n\n# in bash:\n# remove the results folder and run again:\nrm -rf results\nnextflow run ../code/script2.nf  \n\n\n\n\n\n\n\n\n\noutput VS emit\n\n\n\n\noutput: Note - when you specify output, all the files which are there are expected to exist and are going to be in the final result folder. If some file is not produced, nextflow will give an error. This can be circumvented by using the argumant optional: true.\nemit: name - emit will direct those outputs into a channel named PROCESS.name .\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in a similar way in which channels are expendable, so are processes. If you use a process once, it is no longer usable later. Each time you call a process, you use it. So if you want to save an output of a process to two different channels, save it to a channel, then access the different channels within it by calling the emit name (eg out_channel.html and out_channel.zip )\n\n\n\n\n# in workflow: \n  out_channel = FASTQC(infile_channel)\n  println (\"out_html: \") \n  out_channel.html.view()\n // println (\"out_zip: \") \n // out_channel.zip.view()\n\n# in bash:\n# remove the results folder and run again:\nrm -rf results\nnextflow run ../code/script2.nf  \n\n\n\n\nWe have now repeated the fastqc computation a lot of times already. There is a neat option in nextflow which allowes you to save computation time and repeat only the steps which have changed. you can activate it by running nextflow command with -resume tag.\n\n\nnextflow run ../code/script2.nf  -resume\n\nThis way only the steps we change will be ran again, the rest will be found in cache!!\n‚Ä¶ So far so good?\n\n\n\nOf course, you don‚Äôt need to hard code the path, you can use variable that will store a path of the file/files you want to input.\nin the workflow, you can simply replace the actual file path with a parameter defined in params.nameoftheparameter, where nameoftheparameter can be any variable name you choose.\n\n# in the workflow:  \n#  // parse input:\n  infile_channel = Channel.fromPath( params.infile )\n                    .view()\n\nBy using the special word params. nextflow will look at the parameters which were provided when the run was executed. Neat thing about this is that you can define the parameter directly when you call the nextflow run:\n\n\n\n\n\n\nTip\n\n\n\nto define a parameter while callling a nextflow run, simply use double dash: ‚Äò--‚Äô followed by the name of the parameter.\nfor example, to specify a param.infile in workflow, we would need to call\nnextflow run --infile \"path/to/file\" .\nAlternatively, you can create a config file and store your parameters there! - neat for keeping track of runs!\n\n\n\nTry it out!\n\n\nnextflow run ../code/script2.nf -resume --infile \"../../data/*fq\"\n\n\n\n\n\n\n\nThere are many ways to input a file to channel. In bioinformatics, there is often a need to input pairs of files. So, in nextflow there is a special operator for this fromFilePairs .\nLets try this out in script 3: Open and change script3:\n\n\n# workflow {\n\n  // parse input:\n  infile_channel = Channel.fromFilePairs( params.infile )\n                    .view()\n \n\nand call it with:\n\n\nnextflow run ../code/script3.nf -resume --infile \"../../data/*{1,2}.fq\"\n\nWhen using fromFilePairs we need to specify file pairs to input. *{1,2}.fq means the input will be all pairs of files ending with _1.fq and _2.fq and everything before that will be considered sample id.\nThis creates a channel with 3 elements each of which are a tuple of two elements. First one is the sample ID, and second one is a list of two elements, which are paths to file_1.fq and file_2.fq.\nIt is easy to access the individual elements, you can do it in the following way:\nIn the process, it is easy to parse this, just instead of path infilename use:\n\ntuple val(sampleid),path(infiles)\n\nNow we can access individual file from infiles with ${infiles[0]} ${infiles[1]}, and we know the sampleid, it is saved as variable ${sampleid}.\nAnother neat thing you can do is to tag the process execution by some name, lets see this by using ‚Äútag‚Äù:\n\n# process\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n  tag \"running on $sampleid\" \n\n  input:\n  tuple val(sampleid),path(infiles)\n  \n  output:\n  path '*html' \n  publishDir \"results\"\n\n  \n  script:\n  \"\"\"\n  fastqc ${infiles[0]} ${infiles[1]}\n  \"\"\"\n}\n\n\n# workflow, add:\nFASTQC(infile_channel)\n      .view()\n\n# run with: \nnextflow run ../code/script3.nf --infile \"../../data/*{1,2}.fq\" -ansi-log false",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#processes-and-channels",
    "href": "2_channels.html#processes-and-channels",
    "title": "Nextflow intro",
    "section": "",
    "text": "Nextflow workflow is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).\n\nProcesses are executed independently and are isolated from each other. The only way they can communicate is via ‚Äúchannels‚Äù, asynchronous first-in, first-out (FIFO) queues. In other words, every input and output of a process is represented as a channel. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these input and output declarations.",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#my-first-nextflow-script",
    "href": "2_channels.html#my-first-nextflow-script",
    "title": "Nextflow intro",
    "section": "",
    "text": "First nextflow script\n\nmkdir tests\ncd tests\n\nnextflow run ../code/my_first_script.nf \n\n\n\n\n\n\n\nExplanation:\n\n\n\nnextflow run code/my_first_script.nf runs a nextflow script code/my_first_script.nf\n-c config.nf uses configuration file for nextflow run. Parameters which are nextflow-related are handed with a single dash.",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#script-walkthrough",
    "href": "2_channels.html#script-walkthrough",
    "title": "Nextflow intro",
    "section": "",
    "text": "nextflow.enable.dsl=2 # (Domain-Specific Language version 2)\n\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n\n  input:\n  path infilename\n\n  output:\n  path '*' \n      \n  script:\n  \"\"\"\n  fastqc ${infilename}  \n  \"\"\"\n}\n\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromPath( \"../../data/liver_1.fq\" )\n\n  // run FASTQC:\n  FASTQC(infile_channel)\n\n}\n\n\nTo view a content of a channel, you can use the operator view() For example, we created code/script1.nf where we only created a channel from a file. To view it, just add .view() after a channel.\n\n# code/script1.nf\nworkflow {\n\n  // parse input:\n  infile_channel_1 = \n    Channel.fromPath( \"../../data/liver_*.fq\" )\n    .view()\n\n}\n\n\nWe did this in script1 . So lets run it and see what we get:\n\nnextflow run ../code/script1.nf \n\n\n\nThere are two types of channels in nextflow:\nqueue channels and value channels. Queue channels are expendable, they can only be used once - like when you pipe data in bash. Once used, elements in this queue cannot be re-used. Most channels in nextflow are queue channels. Value channels are long-lived and can be re-used, but they always only hold a single value. They usually hold constant values (or absolute paths).\nThere are many ways to create a channel in nextflow, I advise you to look at the official documentation on channel factories for more information.\n\nResults are saved in working directory, two layers down. There, .html and .zip files are created.\n\n\n\nchange script2.nf to: write liver_*.fq instead of liver_1.fq and view the infile_channel.\nwhen process is ran, the outputs are also gathered into a channel. In other words, FASTQC(infile_channel) is also a channel. View the elements in it!\n\n\n\n\n# change ../code/script2.nf and run it.\n# 1. change liver_1.fq to *fq \n# add .view() to view output of the input channel!\n\n# run the script:  \nnextflow run ../code/script2.nf  \n\n# view the outfile channel and run the script again\n\n\n\n\n\n\n\nüí° Solution:\n\n\n\n\n\nCode\nworkflow {\n\n  // parse input:\n  infile_channel = Channel.fromPath( \"../../data/liver_*.fq\" )\n                      .view()\n  // run FASTQC:\n  FASTQC(infile_channel)\n    .view()\n\n}\n\nnextflow run ../code/script2.nf  \n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you add -ansi-log false to nextflow run command, you will see all the processes instead of single process per line!\nnextflow run ../code/script2.nf  -ansi-log false\nNow fastqc is ran 2 times!\n\n\n\n\n\n\n\nNow we created multiple files as results, but they are all somewhere around in the work directory. You can define publishDir to have the resuls all in one place.\nAdd the following line to the process:\n\n#process:\n  publishDir \"results\"\n\n#run in bash: \nnextflow run ../code/script2.nf  \n\nNow the results (liver_1_fastqc.html liver_1_fastqc.zip liver_2_fastqc.html liver_2_fastqc.zip) will be saved in the folder: results/ liver_1_fastqc.html liver_1_fastqc.zip liver_2_fastqc.html liver_2_fastqc.zip\n\n\n\nIf you want to collect only html files, you can specify this in the output by :\n\n#process: \n  output:\n  path \"*html\"\n \n# bash:\n#remove the results folder and run the script again:  \nrm -rf results\nnextflow run ../code/script2.nf  \n\nNow the fastqc process only outputs the html files in the channel, and only they are in the results folder.\nWe have seen already that output is a channel. If we want to direct different outputs into different channels, this can be easily done by specifying where the output will be directed to - using the emit keyword.\n\n#in process:  \n\n  output :\n  path \"*html\"\n  path \"*zip\", emit: zip\n\n\n\n\n\n\n\nWarning\n\n\n\nThis will produce an error:\n\n\n# in workflow: \n  FASTQC(infile_channel).view()\n\n# in bash:\n# remove the results folder and run again:\nrm -rf results\nnextflow run ../code/script2.nf  \n\nOutput produces 2 channels, and nextflow doesnt know which one we want. You should specify which one you are interestred in by choosing .html or .zip. So, do this:\n\n\n# in workflow: \n  FASTQC(infile_channel).zip.view()\n\n# in bash:\n# remove the results folder and run again:\nrm -rf results\nnextflow run ../code/script2.nf  \n\n\n\n\n\n\n\n\n\noutput VS emit\n\n\n\n\noutput: Note - when you specify output, all the files which are there are expected to exist and are going to be in the final result folder. If some file is not produced, nextflow will give an error. This can be circumvented by using the argumant optional: true.\nemit: name - emit will direct those outputs into a channel named PROCESS.name .\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in a similar way in which channels are expendable, so are processes. If you use a process once, it is no longer usable later. Each time you call a process, you use it. So if you want to save an output of a process to two different channels, save it to a channel, then access the different channels within it by calling the emit name (eg out_channel.html and out_channel.zip )\n\n\n\n\n# in workflow: \n  out_channel = FASTQC(infile_channel)\n  println (\"out_html: \") \n  out_channel.html.view()\n // println (\"out_zip: \") \n // out_channel.zip.view()\n\n# in bash:\n# remove the results folder and run again:\nrm -rf results\nnextflow run ../code/script2.nf  \n\n\n\n\nWe have now repeated the fastqc computation a lot of times already. There is a neat option in nextflow which allowes you to save computation time and repeat only the steps which have changed. you can activate it by running nextflow command with -resume tag.\n\n\nnextflow run ../code/script2.nf  -resume\n\nThis way only the steps we change will be ran again, the rest will be found in cache!!\n‚Ä¶ So far so good?\n\n\n\nOf course, you don‚Äôt need to hard code the path, you can use variable that will store a path of the file/files you want to input.\nin the workflow, you can simply replace the actual file path with a parameter defined in params.nameoftheparameter, where nameoftheparameter can be any variable name you choose.\n\n# in the workflow:  \n#  // parse input:\n  infile_channel = Channel.fromPath( params.infile )\n                    .view()\n\nBy using the special word params. nextflow will look at the parameters which were provided when the run was executed. Neat thing about this is that you can define the parameter directly when you call the nextflow run:\n\n\n\n\n\n\nTip\n\n\n\nto define a parameter while callling a nextflow run, simply use double dash: ‚Äò--‚Äô followed by the name of the parameter.\nfor example, to specify a param.infile in workflow, we would need to call\nnextflow run --infile \"path/to/file\" .\nAlternatively, you can create a config file and store your parameters there! - neat for keeping track of runs!\n\n\n\nTry it out!\n\n\nnextflow run ../code/script2.nf -resume --infile \"../../data/*fq\"",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-task2",
    "href": "2_channels.html#exercise-task2",
    "title": "Nextflow intro",
    "section": "",
    "text": "change script2.nf to: write liver_*.fq instead of liver_1.fq and view the infile_channel.\nwhen process is ran, the outputs are also gathered into a channel. In other words, FASTQC(infile_channel) is also a channel. View the elements in it!",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-add-a-trimming-step",
    "href": "2_channels.html#exercise-add-a-trimming-step",
    "title": "Nextflow intro",
    "section": "üí° Exercise: Add a trimming step!",
    "text": "üí° Exercise: Add a trimming step!\nOK, we are ready for a bigger bite!\nCreate a new process, TRIM that will trim the fastq files BEFORE the FASTQ step, as we did before with bash: Use appropriate variables as needed!\nRemember, we used the container: 'biocontainers/fastp:v0.20.1_cv1'.\nThis is what we have from before:\n\n\nfastp\\\n  --in1 data/liver_1.fq\\\n  --in2 data/liver_2.fq\\\n  --out1 data/liver_1.trimmed.fq\\\n  --out2 data/liver_2.trimmed.fq\\\n  -h liver.html",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "bash and containers",
    "section": "",
    "text": "By the end of this workshop you should be able to:\n\nsee the advantages of Nextflow over bash\nWrite a simple Nextflow workflow\nHave basic understanding of the concepts of Channels, Processes and Operators\nRun an test nf-core pipeline\nKnow where to go next ü§∑",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#solution---bash",
    "href": "1_intro.html#solution---bash",
    "title": "bash and containers",
    "section": "Solution - bash",
    "text": "Solution - bash\nIn bash we would do this:\n\nfastp \\\n      --in1 data/liver_1.fq \\\n      --in2 data/liver_2.fq \\\n      --out1 data/liver_1.trimmed.fq \\\n      --out2 data/liver_2.trimmed.fq \\\n      -h liver.html \n\nBut of course, it will fail since we do not have fastp and fastqc installed.\n\n\n\n\n\n\nTip\n\n\n\nGood practice is to use containers which will enable reproducibility. Building dockerfiles and images is sometimes tricky, but also could be defying the purpose of reproducible research. You can find many already available dockers/singularity images at dockerhub and quay.io .",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#use-containers",
    "href": "1_intro.html#use-containers",
    "title": "bash and containers",
    "section": "Use containers!",
    "text": "Use containers!\nWe will use the docker for fastp and fastqc available on dockerhub.\nFirst; pull the container locally:\n\ndocker pull biocontainers/fastp:v0.20.1_cv1\n\nNext, enter the container image:\n\ndocker run -v .:/data -it biocontainers/fastp:v0.20.1_cv1 /bin/bash\n\n# exit the container with exit\n\n\n\n\n\n\n\nExplanation\ndocker run start a new container from a specified image.\n-v .:/data mounts a volume. Here, the current directory (represented by .) on your host machine is mapped to the /data directory inside the container.\n-i (interactive) t (tty) provides a terminal session inside the container.\nTogether, they allow you to run the container in interactive mode, making it easier to execute commands and see outputs.\nbiocontainers/fastp:v0.20.1_cv1 docker being used. image:tag.\n/bin/bash which command to run within a docker.\n\n\n\nOnce you entered the container image, the fastp program exists and we all have the same version. Moreover, we all are running it on the same version of the OS. Check:\n\ncat /etc/os-release\nfastp --version\nexit # to exit the container image\n\nNAME=\"Ubuntu\" VERSION=\"16.04.6 LTS (Xenial Xerus)\"\\\nID=ubuntu\\\nID_LIKE=debian\\\nPRETTY_NAME=\"Ubuntu 16.04.6 LTS\"\\\nVERSION_ID=\"16.04\"\\\nHOME_URL=\"http://www.ubuntu.com/\"\\\nSUPPORT_URL=\"http://help.ubuntu.com/\"\\\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\\\nVERSION_CODENAME=xenial\\\nUBUNTU_CODENAME=xenial\n\nfastp 0.20.1",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#bash-command-with-containers",
    "href": "1_intro.html#bash-command-with-containers",
    "title": "bash and containers",
    "section": "bash command with containers",
    "text": "bash command with containers\nSo, lets combine bash command for fastp and container:\n\ndocker run -v .:/data -it biocontainers/fastp:v0.20.1_cv1 fastp \\\n      --in1 data/liver_1.fq \\\n      --in2 data/liver_2.fq \\\n      --out1 data/liver_1.trimmed.fq \\\n      --out2 data/liver_2.trimmed.fq \\\n      -h liver.html",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#exercise-task1",
    "href": "1_intro.html#exercise-task1",
    "title": "bash and containers",
    "section": "üí° Exercise: task1",
    "text": "üí° Exercise: task1\nTry it yourself!\n\nPull the container biocontainers/fastqc:v0.11.9_cv8\nUse the fastqc on data/liver_1.fq fastqc data/liver_1.fq",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#output-locations",
    "href": "1_intro.html#output-locations",
    "title": "bash and containers",
    "section": "output locations",
    "text": "output locations\nWe need to take care of where the outputs will be located.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#logging",
    "href": "1_intro.html#logging",
    "title": "bash and containers",
    "section": "logging",
    "text": "logging\nWe need to manually create logs if we want to know what happened.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#memory-threads",
    "href": "1_intro.html#memory-threads",
    "title": "bash and containers",
    "section": "memory, threads",
    "text": "memory, threads\nIf we want to allocate different amount of memory and CPUs to each of the processes we make, we need to create separate bash scripts for each process, and define the threads and memory separately.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#process-dependencies",
    "href": "1_intro.html#process-dependencies",
    "title": "bash and containers",
    "section": "process dependencies",
    "text": "process dependencies\nNext, we need to define dependencies - we will have to wait for one script to finish and then the other one can start running. This will depend on where we run the scripts - if we run it locally in bash, we can use wait; if we run it in slurm/pbs/torque, we need to take care of dependencies ourselves.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#parallelism",
    "href": "1_intro.html#parallelism",
    "title": "bash and containers",
    "section": "parallelism",
    "text": "parallelism\nWe need to take care of writing the bash ‚Äúpipeline‚Äù to enable paralel execution. Also we need to take care of running the scripts in parallel on multiple samples.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#resume",
    "href": "1_intro.html#resume",
    "title": "bash and containers",
    "section": "resume",
    "text": "resume\nWe need to take care of status of each of the runs. If some sample fails in one step, we need to find it manually to understand what and where failed.\nWe need to resume the execution of the pipeline manually when some step fails in some sample.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#upscaling-to-production-level",
    "href": "1_intro.html#upscaling-to-production-level",
    "title": "bash and containers",
    "section": "upscaling to production level",
    "text": "upscaling to production level\nThe additional problems arise when you want to go to production-level, and run this pipeline on thousands of samples. You will want to use the cloud for this, and will need to implement the pipeline to work on the cloud.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#documentation",
    "href": "1_intro.html#documentation",
    "title": "bash and containers",
    "section": "documentation",
    "text": "documentation\nWe need to take care of the documentation. What samples were ran? How do we know if they all ran as expected? With which version of the code were they run? We need to keep track of that manually.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-add-outdir-parameter",
    "href": "2_channels.html#exercise-add-outdir-parameter",
    "title": "Nextflow intro",
    "section": "üí° Exercise: Add outdir parameter!",
    "text": "üí° Exercise: Add outdir parameter!\nAdd a parameter ‚Äúoutdir‚Äù to specify where the output location is for the final nextflow results.",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-add-a-new-fastqc_single-step",
    "href": "2_channels.html#exercise-add-a-new-fastqc_single-step",
    "title": "Nextflow intro",
    "section": "üí° Exercise: Add a new FASTQC_SINGLE step!",
    "text": "üí° Exercise: Add a new FASTQC_SINGLE step!\nIf you want extra work, create additional process, FASTQC_SINGLE. Change the parameters of FASTQC to be able to input and process only a single file. Run that process on the output of TRIM step.",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#logging",
    "href": "2_channels.html#logging",
    "title": "Nextflow intro",
    "section": "",
    "text": "Want to see something quite cool?\nEvery nextflow run automatically loggs all of its progress. The main log from, the last run is always saved as .nextflow.log. Check it out!\n\ncat .nextflow.log  \n\nThere are many information, most useful might be seeing per process status:\n\ncat .nextflow.log | grep COMPLETED  \n\nWhatever is completed successfully has the exit status 0. So if you grep everything which does not have exit status 0 you will find all failed processes:\n\n cat .nextflow.log | grep COMPLETED | grep -v \"exit: 0\"\n\nJust adding flags -with-report -with-dag -with-timeline -with-trace enable reporrts on the pipeline generated automatically.\nIf you also want to see all the processes being ran, you can add -ansi-log false\n\nrun ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme -with-report -with-dag -with-timeline -with-trace -ansi-log false\n\n\n\n\n\n\n\n\nNextflow automatically draws a graph of the workflow!\n\n\n\nNextflow tracks all progress for each run! You can easily get information on how much CPUs or memory was actually used in a process, and get a lot of additional details on where the processes were run.\n\n\nEvery process working directory is available. Check one in the report and enter it. In there, you have :\n\n\n\n\n\n\nImportant files\n\n\n\n.command.run: File that sets the stage. It configures the containers and prepares the environment to run the process.\n.command.sh: The actual script being run. If you need to change something, best way is to change the .sh and run with .command.run\n.command.err: Errors outputted by the process.\n.command.log: Log outputted by the process.\n.command.out: Any standard output outputted by the process.\n.exitcode: If 0, everything is ok.",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-break-the-pipeline",
    "href": "2_channels.html#exercise-break-the-pipeline",
    "title": "Nextflow intro",
    "section": "",
    "text": "This might be the most useful exercise of all..\nLets rename one file to save it, and create an empty file instead of it. Then try to rerun the pipeline to see how difficult it will be to understand what happened.\n\nmv ../../data/liver_1.fq ../../data/liver_1.fq_cp \ntouch ../../data/liver_1.fq\nrm -rf .*\nnextflow run ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme -with-report -with-dag -with-timeline -with-trace",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "2_channels.html#exercise-fix-a-broken-pipeline",
    "href": "2_channels.html#exercise-fix-a-broken-pipeline",
    "title": "Nextflow intro",
    "section": "",
    "text": "( This might be the most useful exercise of all.. )",
    "crumbs": [
      "Channels"
    ]
  },
  {
    "objectID": "4_nf-core.html",
    "href": "4_nf-core.html",
    "title": "nf-core",
    "section": "",
    "text": "Ok, ok‚Ä¶\nI agree it got a bit tricky, yes.\nHowever, since all of this is so great and reproducible, there must be tons of people using it and creating reproducible scripts and best practice pipelines for everyone to run!\nWell!",
    "crumbs": [
      "nf-core"
    ]
  },
  {
    "objectID": "4_nf-core.html#what-is-nf-core",
    "href": "4_nf-core.html#what-is-nf-core",
    "title": "nf-core",
    "section": "What is nf-core",
    "text": "What is nf-core",
    "crumbs": [
      "nf-core"
    ]
  },
  {
    "objectID": "4_nf-core.html#scnanoseq-pipeline",
    "href": "4_nf-core.html#scnanoseq-pipeline",
    "title": "nf-core",
    "section": "scnanoseq pipeline",
    "text": "scnanoseq pipeline\nSince later in the week we have a hackathon, and we plan to give it on single cell transcriptomics data, I wanted to see what was available from ‚Äúbest practice‚Äù workflows to analyse this data. One such pipeline is scnanoseq.\nIt is quite easy to run an nf-core pipeline, because those pipelines are readily available on github, and nextflow works great with github, you can just do this:\nmkdir test_scnanoseq\ncd test_scnanoseq\n#nextflow run nf-core/scnanoseq -profile test,docker --outdir . \nnextflow run nf-core/scnanoseq -profile test,singularity --outdir . \nWhile it works, we can check out what the pipeline does:\n\nAs often the case, this pipeline is designed for a specific use case - nanopore long read single cell experiment analysis. Specifically, it supports barcodes 10x_3v3 and 10x_5v2. One of the goals in the hackathon is to adjust and enable this pipeline to be run on real pacbio derived single cell data that we have, and compare results with the ones provided in the paper. Another goal is to change process with a different one.",
    "crumbs": [
      "nf-core"
    ]
  },
  {
    "objectID": "1_intro.html#the-data",
    "href": "1_intro.html#the-data",
    "title": "bash and containers",
    "section": "The data",
    "text": "The data\nThe data we will work with is located in the data/ directory. The materials are located in web_tutorial. Open 1_intro.qmd and render it.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "2_channels.html#script3.nf",
    "href": "2_channels.html#script3.nf",
    "title": "Nextflow intro",
    "section": "",
    "text": "There are many ways to input a file to channel. In bioinformatics, there is often a need to input pairs of files. So, in nextflow there is a special operator for this fromFilePairs .\nLets try this out in script 3: Open and change script3:\n\n\n# workflow {\n\n  // parse input:\n  infile_channel = Channel.fromFilePairs( params.infile )\n                    .view()\n \n\nand call it with:\n\n\nnextflow run ../code/script3.nf -resume --infile \"../../data/*{1,2}.fq\"\n\nWhen using fromFilePairs we need to specify file pairs to input. *{1,2}.fq means the input will be all pairs of files ending with _1.fq and _2.fq and everything before that will be considered sample id.\nThis creates a channel with 3 elements each of which are a tuple of two elements. First one is the sample ID, and second one is a list of two elements, which are paths to file_1.fq and file_2.fq.\nIt is easy to access the individual elements, you can do it in the following way:\nIn the process, it is easy to parse this, just instead of path infilename use:\n\ntuple val(sampleid),path(infiles)\n\nNow we can access individual file from infiles with ${infiles[0]} ${infiles[1]}, and we know the sampleid, it is saved as variable ${sampleid}.\nAnother neat thing you can do is to tag the process execution by some name, lets see this by using ‚Äútag‚Äù:\n\n# process\nprocess FASTQC {\n    \n  container 'biocontainers/fastqc:v0.11.9_cv8'\n  tag \"running on $sampleid\" \n\n  input:\n  tuple val(sampleid),path(infiles)\n  \n  output:\n  path '*html' \n  publishDir \"results\"\n\n  \n  script:\n  \"\"\"\n  fastqc ${infiles[0]} ${infiles[1]}\n  \"\"\"\n}\n\n\n# workflow, add:\nFASTQC(infile_channel)\n      .view()\n\n# run with: \nnextflow run ../code/script3.nf --infile \"../../data/*{1,2}.fq\" -ansi-log false",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "2_channels.html#script4.nf",
    "href": "2_channels.html#script4.nf",
    "title": "Nextflow intro",
    "section": "script4.nf",
    "text": "script4.nf\nOK, cool! We are ready for some exercise!!",
    "crumbs": [
      "Nextflow intro"
    ]
  },
  {
    "objectID": "3_logs.html",
    "href": "3_logs.html",
    "title": "Logs",
    "section": "",
    "text": "Want to see something quite cool?\nEvery nextflow run automatically loggs all of its progress. The main log from, the last run is always saved as .nextflow.log. Check it out!\ncat .nextflow.log\nThere are many information, most useful might be seeing per process status:\ncat .nextflow.log | grep COMPLETED\nWhatever is completed successfully has the exit status 0. So if you grep everything which does not have exit status 0 you will find all failed processes:\ncat .nextflow.log | grep COMPLETED | grep -v \"exit: 0\"\nJust adding flags -with-report -with-dag -with-timeline -with-trace enable reporrts on the pipeline generated automatically.\nIf you also want to see all the processes being ran, you can add -ansi-log false\nrun ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir tryme -with-report -with-dag -with-timeline -with-trace -ansi-log false\nBUT! It needs to be run with a full path!\nso:\nrp=`realpath ../../data/transcriptome.fa`\nnextflow run ../code/script3.nf  -c ../code/config.nf -resume --infile \"../../data/*{1,2}.fq\" --outdir res --transcriptome $rp",
    "crumbs": [
      "Logs"
    ]
  },
  {
    "objectID": "3_logs.html#exercise-fix-a-broken-pipeline",
    "href": "3_logs.html#exercise-fix-a-broken-pipeline",
    "title": "Logs",
    "section": "üí° Exercise: Fix a broken pipeline! üíî",
    "text": "üí° Exercise: Fix a broken pipeline! üíî\n( This might be the most useful exercise of all.. )",
    "crumbs": [
      "Logs"
    ]
  }
]